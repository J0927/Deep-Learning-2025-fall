{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "3bd90bd1-448f-401f-b5e8-f897f918dbd7",
      "metadata": {
        "id": "3bd90bd1-448f-401f-b5e8-f897f918dbd7"
      },
      "source": [
        "# NN-自然語言處理 with Transformer\n",
        "## 教學目標\n",
        "- 本教學著重於自然語言處理，主要涵蓋 `Transformer`。\n",
        "- 這份教學的目標是介紹如何以 Python 和 PyTorch 實作神經網路。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "758e8f94-8fcb-4ed0-a4a3-01d442a196a2",
      "metadata": {
        "id": "758e8f94-8fcb-4ed0-a4a3-01d442a196a2"
      },
      "source": [
        "## 使用 NN 來進行中文的分類任務\n",
        "\n",
        "- 我們將在這個教學裡讓大家實作中文情緒分析（Sentiment Analysis）\n",
        "- 本資料集爲外賣平臺用戶評價分析，[下載連結](https://raw.githubusercontent.com/SophonPlus/ChineseNlpCorpus/master/datasets/waimai_10k/waimai_10k.csv)。\n",
        "- 資料集欄位爲標籤（label）和評價（review），\n",
        "- 標籤 1 爲正向，0 爲負向。\n",
        "- 正向 4000 條，負向約 8000 條。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5fff29fa",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5fff29fa",
        "outputId": "9dcabc43-0319-4221-9083-5abd63d0dd1b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2025-11-03 08:29:20--  https://raw.githubusercontent.com/SophonPlus/ChineseNlpCorpus/master/datasets/waimai_10k/waimai_10k.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 919380 (898K) [text/plain]\n",
            "Saving to: ‘data/waimai_10k.csv’\n",
            "\n",
            "\rdata/waimai_10k.csv   0%[                    ]       0  --.-KB/s               \rdata/waimai_10k.csv 100%[===================>] 897.83K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2025-11-03 08:29:20 (26.6 MB/s) - ‘data/waimai_10k.csv’ saved [919380/919380]\n",
            "\n",
            "Requirement already satisfied: jieba in /usr/local/lib/python3.12/dist-packages (0.42.1)\n"
          ]
        }
      ],
      "source": [
        "# 0. 下載資料與安裝 jieba\n",
        "\n",
        "!mkdir -p data\n",
        "!wget https://raw.githubusercontent.com/SophonPlus/ChineseNlpCorpus/master/datasets/waimai_10k/waimai_10k.csv -O data/waimai_10k.csv\n",
        "!pip install jieba"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "tribal-cradle",
      "metadata": {
        "id": "tribal-cradle"
      },
      "outputs": [],
      "source": [
        "# 1. 導入所需套件\n",
        "import math\n",
        "\n",
        "# 第3方套件\n",
        "import jieba\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "floral-particular",
      "metadata": {
        "id": "floral-particular"
      },
      "outputs": [],
      "source": [
        "# 2. 以 pandas 讀取資料\n",
        "# 請先下載資料集\n",
        "\n",
        "df = pd.read_csv(\"./data/waimai_10k.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5764cbb8-a764-43a6-a84d-f20e5b38c209",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "5764cbb8-a764-43a6-a84d-f20e5b38c209",
        "outputId": "7c8fb15b-1d50-44eb-c57c-2d0fecabf44d"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 11987,\n  \"fields\": [\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"review\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 11980,\n        \"samples\": [\n          \"\\u7279\\u5927\\u676f\\u661f\\u51b0\\u4e50\\u548c\\u5927\\u676f\\u7684\\u62ff\\u94c1\\u4e00\\u4e2a\\u676f\\u578b\\uff0c\\u4e0b\\u6b21\\u8981\\u5f53\\u9762\\u786e\\u8ba4\\uff01\",\n          \"\\u4e2d\\u5348\\u5fd9\\u9001\\u9910\\u6162\\u53ef\\u4ee5\\u7406\\u89e3\\uff0c\\u5473\\u9053\\u4e00\\u822c\\u4e5f\\u53ef\\u4ee5\\u7406\\u89e3\\uff0c\\u4f46\\u7ca5\\u91cc\\u9762\\u957f\\u957f\\u7684\\u94a2\\u4e1d\\u7b97\\u600e\\u4e48\\u56de\\u4e8b\\uff1f\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "df"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-c9db1f82-c677-432e-8f6b-26fd49634c68\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>review</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>很快，好吃，味道足，量大</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>没有送水没有送水没有送水</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>非常快，态度好。</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>方便，快捷，味道可口，快递给力</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>菜味道很棒！送餐很及时！</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c9db1f82-c677-432e-8f6b-26fd49634c68')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-c9db1f82-c677-432e-8f6b-26fd49634c68 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-c9db1f82-c677-432e-8f6b-26fd49634c68');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-41bc9c5f-f8cb-4ae8-be4e-55095457c57d\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-41bc9c5f-f8cb-4ae8-be4e-55095457c57d')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-41bc9c5f-f8cb-4ae8-be4e-55095457c57d button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "   label           review\n",
              "0      1     很快，好吃，味道足，量大\n",
              "1      1     没有送水没有送水没有送水\n",
              "2      1         非常快，态度好。\n",
              "3      1  方便，快捷，味道可口，快递给力\n",
              "4      1     菜味道很棒！送餐很及时！"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 3. 觀察資料\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "55c6f14d-298f-435f-b361-d418be65de83",
      "metadata": {
        "id": "55c6f14d-298f-435f-b361-d418be65de83"
      },
      "source": [
        "## 建立字典\n",
        "- 電腦無法僅透過字符來區分不同字之間的意涵\n",
        "- 電腦視覺領域依賴的是影像資料本身的像素值\n",
        "- 我們讓電腦理解文字的方法是透過向量\n",
        "- 文字的意義藉由向量來進行表達的形式稱為 word embeddings\n",
        "- 舉例:\n",
        "$\\textrm{apple}=[0.123, 0.456,0.789,\\dots,0.111]$\n",
        "\n",
        "- 如何建立每個文字所屬的向量？\n",
        "    - 傳統方法: 計數法則\n",
        "    - 近代方法 (2013-至今): 使用(淺層)神經網路訓練 word2vec ([參考](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/))，稱為 word embeddings\n",
        "    - 現代方法 (2018-至今): 使用(深層)神經網路訓練 Transformers，也就是BERT ([參考](https://youtu.be/gh0hewYkjgo))，又稱為 contexualized embeddings\n",
        "- 在那之前，要先建立分散式字詞的字典\n",
        "    - 可粗分兩種斷詞方式 (tokenization):\n",
        "        1. 每個字都斷 (character-level)\n",
        "        2. 斷成字詞 (word-level)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db1b10a5-e5cc-4151-baf4-00852493ab6e",
      "metadata": {
        "id": "db1b10a5-e5cc-4151-baf4-00852493ab6e"
      },
      "source": [
        "## Word embeddings\n",
        "- 著名的方法有:\n",
        "    1. word2vec: Skip-gram, CBOW (continuous bag-of-words)\n",
        "    2. GloVe\n",
        "    3. fastText\n",
        "- 本教學使用 PyTorch 內建的 Embedding 層來實作 word embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "20845258",
      "metadata": {
        "id": "20845258"
      },
      "outputs": [],
      "source": [
        "word_to_idx = {\"<pad>\": 0, \"<unk>\": 1, \"好吃\": 2, \"棒\": 3, \"给力\": 4}\n",
        "embeds = torch.nn.Embedding(5, 5)  # 5 words in vocab, 5 dimensional embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6aec1001-2ee2-4b15-bf3f-588d3320c848",
      "metadata": {
        "id": "6aec1001-2ee2-4b15-bf3f-588d3320c848"
      },
      "outputs": [],
      "source": [
        "def get_word_id(word, vocab, unk_idx: int = 1):\n",
        "    return vocab.get(word, unk_idx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23a1e9e0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23a1e9e0",
        "outputId": "65a5c2a4-4333-4a62-bfe7-b382053f910a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[-0.2812, -0.3170, -0.1047,  0.2052, -0.7845],\n",
            "        [ 0.3511,  0.4559, -1.9599,  1.9738,  0.4159],\n",
            "        [ 0.3511,  0.4559, -1.9599,  1.9738,  0.4159]],\n",
            "       grad_fn=<EmbeddingBackward0>)\n"
          ]
        }
      ],
      "source": [
        "lookup_tensor = torch.tensor(\n",
        "    [\n",
        "        get_word_id(\"好吃\", word_to_idx),\n",
        "        get_word_id(\"不棒\", word_to_idx),\n",
        "        get_word_id(\"<unk>\", word_to_idx),\n",
        "    ],\n",
        ")\n",
        "word_embed = embeds(lookup_tensor)\n",
        "print(word_embed)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7b1fe18f",
      "metadata": {
        "id": "7b1fe18f"
      },
      "source": [
        "### 複習 torch.nn.Linear 用法"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0fc699bc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0fc699bc",
        "outputId": "fc2b35e9-ff18-440c-e6f3-259771f6e7f1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([128, 30])\n"
          ]
        }
      ],
      "source": [
        "# torch.nn.Linear 用法範例\n",
        "# m 是一個線性轉換層，之前我們都在 forward 裡面使用它\n",
        "\n",
        "m = torch.nn.Linear(20, 30)\n",
        "input = torch.randn(128, 20) # 假設有 128 筆資料，每筆資料有 20 維\n",
        "output = m(input)\n",
        "\n",
        "print(output.size())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d20a4d6-103e-4e9e-8ca7-d18c7750c1be",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4d20a4d6-103e-4e9e-8ca7-d18c7750c1be",
        "outputId": "10712755-836c-4ad0-ace8-7c6b1b367a34"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Building prefix dict from the default dictionary ...\n",
            "DEBUG:jieba:Building prefix dict from the default dictionary ...\n",
            "Dumping model to file cache /tmp/jieba.cache\n",
            "DEBUG:jieba:Dumping model to file cache /tmp/jieba.cache\n",
            "Loading model cost 0.627 seconds.\n",
            "DEBUG:jieba:Loading model cost 0.627 seconds.\n",
            "Prefix dict has been built successfully.\n",
            "DEBUG:jieba:Prefix dict has been built successfully.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The vocab size is 11010.\n"
          ]
        }
      ],
      "source": [
        "# 4. 建立字典\n",
        "use_jieba=True\n",
        "\n",
        "vocab = {'<pad>':0, '<unk>':1}\n",
        "\n",
        "if use_jieba:\n",
        "    words = []\n",
        "    for sent in df['review']:\n",
        "        tokens = jieba.lcut(sent, cut_all=False)\n",
        "        words.extend(tokens)\n",
        "\n",
        "else:\n",
        "    # 以 character-level 斷詞\n",
        "    words = df['review'].str.cat()\n",
        "\n",
        "# 使字詞不重複\n",
        "words = sorted(set(words))\n",
        "for idx, word in enumerate(words):\n",
        "    # 一開始已經放兩個進去 dictionary 了\n",
        "    idx = idx + 2\n",
        "    # 將 word to id 放到 dictionary\n",
        "    vocab[word] = idx\n",
        "\n",
        "# 查看字典大小\n",
        "print(\"The vocab size is {}.\".format(len(vocab)))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "035a31d6-38d3-44f2-a98d-d487b2a90987",
      "metadata": {
        "id": "035a31d6-38d3-44f2-a98d-d487b2a90987"
      },
      "source": [
        "## 使用 PyTorch 建立 Dataset\n",
        "![Imgur](https://i.imgur.com/wGnfCmH.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e40cb6e1-bd26-44c2-886c-b3651f204d50",
      "metadata": {
        "id": "e40cb6e1-bd26-44c2-886c-b3651f204d50",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# 5. 將資料分成 train/ validation/ test\n",
        "\n",
        "train_data, test_data = train_test_split(\n",
        "    df,\n",
        "    test_size=0.2,\n",
        ")\n",
        "train_data, validation_data = train_test_split(\n",
        "    train_data,\n",
        "    test_size=0.1,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "nIMO1txA2_AP",
      "metadata": {
        "id": "nIMO1txA2_AP"
      },
      "source": [
        "## 定義超參數\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c81e62ed-13ad-483c-a47a-0407d7b3ef35",
      "metadata": {
        "id": "c81e62ed-13ad-483c-a47a-0407d7b3ef35"
      },
      "outputs": [],
      "source": [
        "# 6. 定義超參數\n",
        "\n",
        "parameters = {\n",
        "    \"padding_idx\": 0,\n",
        "    \"vocab_size\": len(vocab),\n",
        "    # Hyperparameters\n",
        "    \"embed_dim\": 300,\n",
        "    \"hidden_dim\": 256,\n",
        "    \"module_name\": 'rnn', # 選項: rnn, lstm, gru, transformer\n",
        "    \"num_layers\": 2,\n",
        "    \"learning_rate\": 5e-4, # 使用 Transformer 時建議改成 5e-5\n",
        "    \"epochs\": 10,\n",
        "    \"max_seq_len\": 50,\n",
        "    \"batch_size\": 64,\n",
        "    \"bidirectional\": True,\n",
        "    # Transformers\n",
        "    \"num_heads\": 4,\n",
        "    \"dropout\": 0.2,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "standard-joining",
      "metadata": {
        "id": "standard-joining"
      },
      "outputs": [],
      "source": [
        "# 7. 建立 PyTorch Dataset (定義 class)\n",
        "\n",
        "class WaimaiDataset(torch.utils.data.Dataset):\n",
        "    # 繼承 torch.utils.data.Dataset\n",
        "    def __init__(self, vocab, data, max_seq_len, use_jieba):\n",
        "        self.df = data\n",
        "        self.max_seq_len = max_seq_len\n",
        "        # 可以選擇要不要使用結巴進行斷詞\n",
        "        self.use_jieba = use_jieba\n",
        "        self.vocab = vocab\n",
        "        self.unk_idx = self.vocab.get('<unk>')\n",
        "\n",
        "    # 改寫繼承的 __getitem__ function\n",
        "    def __getitem__(self, idx):\n",
        "        # dataframe 的第一個 column 是 label\n",
        "        # dataframe 的第一個 column 是 評論的句子\n",
        "        label, sent = self.df.iloc[idx, 0:2]\n",
        "        # 先將 label 轉為 float32 以方便後面進行 loss function 的計算\n",
        "        label_tensor = torch.tensor(label, dtype=torch.float32)\n",
        "        if self.use_jieba:\n",
        "            # 使用 lcut 可以 return list\n",
        "            tokens = jieba.lcut(sent, cut_all=False)\n",
        "        else:\n",
        "            # 每個字都斷詞\n",
        "            tokens = list(sent)\n",
        "\n",
        "        # 控制最大的序列長度\n",
        "        tokens = tokens[:self.max_seq_len]\n",
        "\n",
        "        # 根據 vocab 轉換 word id\n",
        "        # 如果找不到該字詞，就用 <unk> 的 index 來表示\n",
        "        tokens_id = [self.vocab.get(word, self.unk_idx) for word in tokens]\n",
        "        tokens_tensor = torch.LongTensor(tokens_id)\n",
        "\n",
        "        # 所以 第 0 個index是句子，第 1 個index是 label\n",
        "        return tokens_tensor, label_tensor\n",
        "\n",
        "    # 改寫繼承的 __len__ function\n",
        "    def __len__(self):\n",
        "        return len(self.df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee767846-421c-4eff-a4ff-3986308f1497",
      "metadata": {
        "id": "ee767846-421c-4eff-a4ff-3986308f1497"
      },
      "outputs": [],
      "source": [
        "# 8. 建立 PyTorch Dataset (執行 class)\n",
        "use_jieba=use_jieba\n",
        "\n",
        "trainset = WaimaiDataset(\n",
        "    vocab,\n",
        "    train_data,\n",
        "    parameters[\"max_seq_len\"],\n",
        "    use_jieba=use_jieba\n",
        ")\n",
        "validset = WaimaiDataset(\n",
        "    vocab,\n",
        "    validation_data,\n",
        "    parameters[\"max_seq_len\"],\n",
        "    use_jieba=use_jieba\n",
        ")\n",
        "testset = WaimaiDataset(\n",
        "    vocab,\n",
        "    test_data,\n",
        "    parameters[\"max_seq_len\"],\n",
        "    use_jieba=use_jieba\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d98e478-770e-4d23-9f76-b20b3b88b906",
      "metadata": {
        "id": "3d98e478-770e-4d23-9f76-b20b3b88b906"
      },
      "outputs": [],
      "source": [
        "# 9. 整理 batch 的資料 (定義 function)\n",
        "\n",
        "def collate_batch(batch):\n",
        "    # 抽每一個 batch 的第 0 個(注意順序)\n",
        "    text = [i[0] for i in batch]\n",
        "    # 進行 padding\n",
        "    text = pad_sequence(text, batch_first=True)\n",
        "\n",
        "    # 抽每一個 batch 的第 1 個(注意順序)\n",
        "    label = [i[1] for i in batch]\n",
        "    # 把每一個 batch 的答案疊成一個 tensor\n",
        "    label = torch.stack(label)\n",
        "\n",
        "    return text, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4540bea8-399e-4759-ab63-351040f20042",
      "metadata": {
        "id": "4540bea8-399e-4759-ab63-351040f20042"
      },
      "outputs": [],
      "source": [
        "# 10. 建立資料分批 (mini-batches)\n",
        "\n",
        "# 因為會針對 trainloader 進行 shuffle\n",
        "# 對 trainloader 進行 shuffle 有助於降低 overfitting\n",
        "\n",
        "trainloader = DataLoader(\n",
        "    trainset,\n",
        "    batch_size=parameters[\"batch_size\"],\n",
        "    collate_fn=collate_batch,\n",
        "    shuffle=True,\n",
        ")\n",
        "validloader = DataLoader(\n",
        "    validset,\n",
        "    batch_size=parameters[\"batch_size\"],\n",
        "    collate_fn=collate_batch,\n",
        "    shuffle=False,\n",
        ")\n",
        "testloader = DataLoader(\n",
        "    testset,\n",
        "    batch_size=parameters[\"batch_size\"],\n",
        "    collate_fn=collate_batch,\n",
        "    shuffle=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c4670a6c-7747-423e-9008-293b44b8a79c",
      "metadata": {
        "id": "c4670a6c-7747-423e-9008-293b44b8a79c"
      },
      "source": [
        "## 建立Transformer模型\n",
        "- 如上課所述，[Transformer](https://arxiv.org/abs/1706.03762) 有 encoder 部分和 decoder 部分，本教學實作encoder部分\n",
        "- PyTorch 已經幫我們實作好 [`torch.nn.TransformerEncoderLayer`](https://docs.pytorch.org/docs/stable/generated/torch.nn.TransformerEncoderLayer.html) 和 [`torch.nn.TransformerEncoder`](https://docs.pytorch.org/docs/stable/generated/torch.nn.TransformerEncoder.html)\n",
        "    - `torch.nn.TransformerEncoderLayer` 代表 encoder 中每一層的設定，例如 `d_model`, `nhead` 等等\n",
        "    - 處理完 encoder 層的設定之後，再用 `torch.nn.TransformerEncoder`來定義層數，以建構一個完整的 Transformer encoder，例如：\n",
        "    ```python\n",
        "    encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8)\n",
        "    transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=6)\n",
        "    src = torch.rand(10, 32, 512)\n",
        "    out = transformer_encoder(src)\n",
        "    ```\n",
        "- 如果你只想要實作 Transformer decoder 的話，使用方法與 `TransformerEncoder` 相似，採用 [`torch.nn.TransformerDecoderLayer`](https://docs.pytorch.org/docs/stable/generated/torch.nn.TransformerDecoderLayer.html) 與 [`torch.nn.TransformerDecoder`](https://docs.pytorch.org/docs/stable/generated/torch.nn.TransformerDecoder.html)\n",
        "- 如果你想要直接建立一個 Transformer，可以使用 [`torch.nn.Transformer`](https://docs.pytorch.org/docs/stable/generated/torch.nn.Transformer.html#torch.nn.Transformer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vlKBAWax2NO5",
      "metadata": {
        "id": "vlKBAWax2NO5"
      },
      "outputs": [],
      "source": [
        "# 11-1. 建立Transformer模型的位置編碼\n",
        "\n",
        "class PositionalEncoding(torch.nn.Module):\n",
        "    r\"\"\"Inject some information about the relative or absolute position of the tokens\n",
        "        in the sequence. The positional encodings have the same dimension as\n",
        "        the embeddings, so that the two can be summed. Here, we use sine and cosine\n",
        "        functions of different frequencies.\n",
        "\n",
        "    Args:\n",
        "        d_model: the embed dim (required).\n",
        "        dropout: the dropout value (default=0.1).\n",
        "        max_len: the max. length of the incoming sequence (default=5000).\n",
        "    Examples:\n",
        "        >>> pos_encoder = PositionalEncoding(d_model)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
        "        super().__init__()\n",
        "        self.dropout = torch.nn.Dropout(p=dropout)\n",
        "\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        r\"\"\"Inputs of forward function\n",
        "        Args:\n",
        "            x: the sequence fed to the positional encoder model (required).\n",
        "        Shape:\n",
        "            x: [sequence length, batch size, embed dim]\n",
        "            output: [sequence length, batch size, embed dim]\n",
        "        Examples:\n",
        "            >>> output = pos_encoder(x)\n",
        "        \"\"\"\n",
        "\n",
        "        x = x + self.pe[:x.size(0), :]\n",
        "        return self.dropout(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "W1lBb0bB0VPD",
      "metadata": {
        "id": "W1lBb0bB0VPD"
      },
      "outputs": [],
      "source": [
        "# 12-1. 建立Transformer encoder模型\n",
        "\n",
        "class CustomTransformerEncoder(torch.nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size: int,\n",
        "        embed_dim: int,\n",
        "        hidden_dim: int,\n",
        "        padding_idx: int,\n",
        "        num_layers: int,\n",
        "        num_heads: int,\n",
        "        dropout: float,\n",
        "    ):\n",
        "        \"\"\"定義能夠處理句子分類任務的 Transformer encoder 模型架構\n",
        "        Arguments:\n",
        "            - args (dict): 所需要的模型參數 (parameters)\n",
        "        Returns:\n",
        "            - None\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        # 定義 Embedding 層\n",
        "        self.embedding_layer = torch.nn.Embedding(\n",
        "            num_embeddings=vocab_size,\n",
        "            embedding_dim=embed_dim,\n",
        "            padding_idx=padding_idx\n",
        "        )\n",
        "        # 定義 dropout 層\n",
        "        self.embedding_dropout = torch.nn.Dropout(dropout)\n",
        "\n",
        "        # 定義 Positional Encoding (位置編碼)\n",
        "        self.pos_encoder = PositionalEncoding(\n",
        "            d_model=embed_dim,\n",
        "            dropout=dropout\n",
        "        )\n",
        "        encoder_layer = torch.nn.TransformerEncoderLayer(\n",
        "            d_model=embed_dim,\n",
        "            nhead=num_heads,\n",
        "            dim_feedforward=hidden_dim,\n",
        "            dropout=dropout\n",
        "        )\n",
        "        self.transformer_encoder = torch.nn.TransformerEncoder(\n",
        "            encoder_layer=encoder_layer,\n",
        "            num_layers=num_layers\n",
        "        )\n",
        "        self.linear_layer = torch.nn.Linear(\n",
        "            in_features=embed_dim,\n",
        "            out_features=embed_dim\n",
        "        )\n",
        "        self.output_layer = torch.nn.Linear(\n",
        "            in_features=embed_dim,\n",
        "            out_features=1\n",
        "        )\n",
        "\n",
        "    def forward(self, X):\n",
        "        \"\"\"定義神經網路的前向傳遞的進行流程\n",
        "        Arguments:\n",
        "            - X: 輸入值，維度為 [B, S]，其中 B 為 batch size，S 為 sequence length\n",
        "        Returns:\n",
        "            - logits: 模型的輸出值，維度為(B, 1)，其中 B 為 batch size\n",
        "            - Y: 模型的輸出值但經過非線性轉換 (這邊是用 sigmoid)，維度為(B, 1)，其中 B 為 batch size\n",
        "        \"\"\"\n",
        "        # B: batch size; S: sequence length; E: embedding dimension\n",
        "        E = self.embedding_layer(X) # 輸出維度: [B, S, E]\n",
        "        E = self.embedding_dropout(E)\n",
        "\n",
        "        # 加入位置編碼\n",
        "        E = self.pos_encoder(E) # 輸出維度為 (B, S, E)\n",
        "\n",
        "        # 使用 Transformer\n",
        "        # PyTorch 官方 Transformer 預設是 seq-first (seq_len, batch, d_model)\n",
        "        # 所以我們要先交換一下維度資訊\n",
        "        E = E.transpose(0, 1) # 輸出維度為 (S, B, E)\n",
        "\n",
        "        E = self.transformer_encoder(E) # 輸出維度為 (S, B, E)\n",
        "\n",
        "        # 等等要經過分類層，所以要再轉回 (B, S, E)\n",
        "        E = E.transpose(0, 1) # 輸出維度為 (B, S, E)\n",
        "\n",
        "        H_out = self.linear_layer(E) # 輸出維度為 (B, S, E)\n",
        "\n",
        "        # 取第一個 hidden state\n",
        "        logits = self.output_layer(H_out[:, 0, :]) # 輸出維度為 (B, 1, E)\n",
        "        Y = torch.sigmoid(logits)\n",
        "\n",
        "        return logits, Y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f580c1a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8f580c1a",
        "outputId": "ef1d0a5a-7a7b-4692-dea7-4d12f705cd52"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# 12. 執行訓練所需要的準備工作\n",
        "\n",
        "# model = RNNModel(\n",
        "#     vocab_size=parameters[\"vocab_size\"],\n",
        "#     embed_dim=parameters[\"embed_dim\"],\n",
        "#     hidden_dim=parameters[\"hidden_dim\"],\n",
        "#     padding_idx=parameters[\"padding_idx\"],\n",
        "#     bi=parameters[\"bidirectional\"],\n",
        "# )\n",
        "\n",
        "model = CustomTransformerEncoder(\n",
        "    vocab_size=parameters[\"vocab_size\"],\n",
        "    embed_dim=parameters[\"embed_dim\"],\n",
        "    hidden_dim=parameters[\"hidden_dim\"],\n",
        "    padding_idx=parameters[\"padding_idx\"],\n",
        "    num_layers=parameters[\"num_layers\"],\n",
        "    num_heads=parameters[\"num_heads\"],\n",
        "    dropout=parameters[\"dropout\"],\n",
        ")\n",
        "\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "model.to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=parameters[\"learning_rate\"])\n",
        "loss_func = torch.nn.BCEWithLogitsLoss() # 含有 sigmoid 的版本"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f0fcde55-fd80-4995-bce2-cd5e2092d5e9",
      "metadata": {
        "id": "f0fcde55-fd80-4995-bce2-cd5e2092d5e9"
      },
      "source": [
        "## 設定訓練流程"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "07b2579d-b998-4d48-9ad3-52f780823872",
      "metadata": {
        "id": "07b2579d-b998-4d48-9ad3-52f780823872"
      },
      "outputs": [],
      "source": [
        "# 13. 設定訓練流程 (定義 function)\n",
        "\n",
        "def train(trainloader, model, optimizer, loss_func):\n",
        "    \"\"\"定義訓練時的進行流程\n",
        "    Arguments:\n",
        "        - trainloader: 具備 mini-batches 的 dataset，由 PyTorch DataLoader 所建立\n",
        "        - model: 要進行訓練的模型\n",
        "        - optimizer: 最佳化目標函數的演算法\n",
        "    Returns:\n",
        "        - train_loss: 模型在一個 epoch 的 training loss\n",
        "    \"\"\"\n",
        "    # 設定模型的訓練模式\n",
        "    model.train()\n",
        "\n",
        "    # 記錄一個 epoch中 training 過程的 loss\n",
        "    train_loss = 0\n",
        "    # 從 trainloader 一次一次抽\n",
        "    for x, y in tqdm(trainloader, desc=\"Training\"):\n",
        "        # 將變數丟到指定的裝置位置\n",
        "        x = x.to(device)\n",
        "        y = y.to(device)\n",
        "\n",
        "        # 重新設定模型的梯度\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # 1. 前向傳遞 (Forward Pass)\n",
        "        logits, pred = model(x)\n",
        "\n",
        "        # 2. 計算 loss (loss function 為二元交叉熵)\n",
        "        loss = loss_func(logits.squeeze(-1), y)\n",
        "\n",
        "        # 3. 計算反向傳播的梯度\n",
        "        loss.backward()\n",
        "        # 4. \"更新\"模型的權重\n",
        "        optimizer.step()\n",
        "\n",
        "        # 一個 epoch 會抽很多次 batch，所以每個 batch 計算完都要加起來\n",
        "        # .item() 在 PyTorch 中可以獲得該 tensor 的數值\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    return train_loss / len(trainloader)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a4ba00e6-0263-4b81-be42-359de3257859",
      "metadata": {
        "id": "a4ba00e6-0263-4b81-be42-359de3257859"
      },
      "source": [
        "## 設定驗證流程"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e751e9b5-7628-4320-8b20-917bb0f3087f",
      "metadata": {
        "id": "e751e9b5-7628-4320-8b20-917bb0f3087f"
      },
      "outputs": [],
      "source": [
        "# 14. 設定驗證流程 (定義 function)\n",
        "\n",
        "def evaluate(dataloader, model, loss_func):\n",
        "    \"\"\"定義驗證時的進行流程\n",
        "    Arguments:\n",
        "        - dataloader: 具備 mini-batches 的 dataset，由 PyTorch DataLoader 所建立\n",
        "        - model: 要進行驗證的模型\n",
        "    Returns:\n",
        "        - loss: 模型在驗證/測試集的 loss\n",
        "        - acc: 模型在驗證/測試集的正確率\n",
        "    \"\"\"\n",
        "    # 設定模型的驗證模式\n",
        "    # 此時 dropout 會自動關閉\n",
        "    model.eval()\n",
        "    total_loss = 0 # 紀錄 loss 數值\n",
        "    label_list = []\n",
        "    prediction_list = []\n",
        "\n",
        "    # 設定現在不計算梯度\n",
        "    with torch.no_grad():\n",
        "        # 從 dataloader 一次一次抽\n",
        "        for x, y in tqdm(dataloader, desc=\"Evaluating\"):\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            logits, pred = model(x)\n",
        "\n",
        "            # 計算 loss (loss function 為二元交叉熵)\n",
        "            # 模型輸出的維度是 (B, 1)，使用.squeeze(-1)可以讓維度變 (B,)\n",
        "            loss = loss_func(logits.squeeze(-1), y)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # 預測的數值大於 0.5 則視為類別1，反之為類別0\n",
        "            pred = (pred > 0.5) * 1 # pred.shape: (B, 1)\n",
        "            prediction_list.extend(pred.cpu().squeeze(-1).tolist())\n",
        "            label_list.extend(y.cpu().tolist())\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    # 計算正確率\n",
        "    acc = accuracy_score(label_list, prediction_list)\n",
        "\n",
        "    return avg_loss, acc"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd9557c9-c97d-4111-be93-2f256cd113f0",
      "metadata": {
        "id": "dd9557c9-c97d-4111-be93-2f256cd113f0"
      },
      "source": [
        "## 開始訓練"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "694f6cfd-9002-4fb5-8e98-f73e267970ba",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "694f6cfd-9002-4fb5-8e98-f73e267970ba",
        "outputId": "3172fbf5-afb8-421f-afaa-16cbbf0c80b1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 135/135 [00:04<00:00, 28.71it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training loss at epoch 1 is 0.5304086109002432.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 135/135 [00:04<00:00, 30.08it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training loss at epoch 2 is 0.39762211199159975.\n",
            "=====Start validation=====\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|██████████| 15/15 [00:00<00:00, 33.66it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation accuracy at epoch 2 is 0.8633993743482794, and validation loss is 0.3493048369884491.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 135/135 [00:04<00:00, 27.14it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training loss at epoch 3 is 0.3617438483017462.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 135/135 [00:04<00:00, 29.82it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training loss at epoch 4 is 0.3428898274898529.\n",
            "=====Start validation=====\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|██████████| 15/15 [00:00<00:00, 44.98it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation accuracy at epoch 4 is 0.8748696558915537, and validation loss is 0.32384509444236753.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 135/135 [00:05<00:00, 26.83it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training loss at epoch 5 is 0.3200523402955797.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 135/135 [00:04<00:00, 29.92it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training loss at epoch 6 is 0.31670454144477844.\n",
            "=====Start validation=====\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|██████████| 15/15 [00:00<00:00, 42.83it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation accuracy at epoch 6 is 0.872784150156413, and validation loss is 0.314593979716301.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 135/135 [00:05<00:00, 24.88it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training loss at epoch 7 is 0.29816480963318437.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 135/135 [00:04<00:00, 29.29it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training loss at epoch 8 is 0.29653940465715195.\n",
            "=====Start validation=====\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|██████████| 15/15 [00:00<00:00, 41.68it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation accuracy at epoch 8 is 0.8738269030239834, and validation loss is 0.33724465370178225.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 135/135 [00:04<00:00, 29.47it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training loss at epoch 9 is 0.2797621112178873.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 135/135 [00:04<00:00, 27.16it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training loss at epoch 10 is 0.2762188545531697.\n",
            "=====Start validation=====\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|██████████| 15/15 [00:00<00:00, 42.64it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation accuracy at epoch 10 is 0.8894681960375391, and validation loss is 0.31517145534356433.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# 15. 整個訓練及驗證過程的 script\n",
        "\n",
        "train_loss_history = []\n",
        "valid_loss_history = []\n",
        "\n",
        "for epoch in range(parameters[\"epochs\"]):\n",
        "    train_loss = train(\n",
        "        trainloader,\n",
        "        model,\n",
        "        optimizer=optimizer,\n",
        "        loss_func=loss_func,\n",
        "    )\n",
        "\n",
        "    print(\"Training loss at epoch {} is {}.\".format(epoch+1, train_loss))\n",
        "    train_loss_history.append(train_loss)\n",
        "\n",
        "    if epoch % 2 == 1:\n",
        "        print(\"=====Start validation=====\")\n",
        "        valid_loss, valid_acc = evaluate(\n",
        "            dataloader=validloader,\n",
        "            model=model,\n",
        "            loss_func=loss_func,\n",
        "        )\n",
        "        valid_loss_history.append(valid_loss)\n",
        "        print(\"Validation accuracy at epoch {} is {}, and validation loss is {}.\"\\\n",
        "              .format(epoch+1, valid_acc, valid_loss))\n",
        "\n",
        "    torch.save(model.state_dict(), \"model_epoch_{}.pkl\".format(epoch))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08cafd4e-a1e7-4fcf-9fdd-7dec429fb1d2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "08cafd4e-a1e7-4fcf-9fdd-7dec429fb1d2",
        "outputId": "b3a548e4-2f2f-4a4d-a33b-e343d56ff916"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=====Start testing=====\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|██████████| 38/38 [00:00<00:00, 39.76it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing accuracy is 0.8669724770642202.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# 16. 預測測試集\n",
        "\n",
        "best_epoch = np.argmin(valid_loss_history)\n",
        "model.load_state_dict(\n",
        "    torch.load(\"model_epoch_{}.pkl\".format(best_epoch))\n",
        ")\n",
        "\n",
        "print(\"=====Start testing=====\")\n",
        "test_loss, test_acc = evaluate(testloader, model, loss_func)\n",
        "print(\"Testing accuracy is {}.\".format(test_acc))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "KrerBgTT2CWB",
      "metadata": {
        "id": "KrerBgTT2CWB"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}

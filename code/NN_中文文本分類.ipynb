{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3bd90bd1-448f-401f-b5e8-f897f918dbd7",
   "metadata": {
    "id": "3bd90bd1-448f-401f-b5e8-f897f918dbd7"
   },
   "source": [
    "# NN-自然語言處理\n",
    "## 教學目標\n",
    "- 本教學著重於自然語言處理，主要涵蓋 `RNN`。\n",
    "- 這份教學的目標是介紹如何以 Python 和 PyTorch 實作神經網路。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758e8f94-8fcb-4ed0-a4a3-01d442a196a2",
   "metadata": {
    "id": "758e8f94-8fcb-4ed0-a4a3-01d442a196a2"
   },
   "source": [
    "## 使用 NN 來進行中文的分類任務\n",
    "\n",
    "- 我們將在這個教學裡讓大家實作中文情緒分析（Sentiment Analysis）\n",
    "- 本資料集爲外賣平臺用戶評價分析，[下載連結](https://raw.githubusercontent.com/SophonPlus/ChineseNlpCorpus/master/datasets/waimai_10k/waimai_10k.csv)。\n",
    "- 資料集欄位爲標籤（label）和評價（review），\n",
    "- 標籤 1 爲正向，0 爲負向。\n",
    "- 正向 4000 條，負向約 8000 條。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5fff29fa",
   "metadata": {
    "id": "5fff29fa"
   },
   "outputs": [],
   "source": [
    "# 0. 下載資料與安裝 jieba\n",
    "\n",
    "!mkdir -p data\n",
    "!wget https://raw.githubusercontent.com/SophonPlus/ChineseNlpCorpus/master/datasets/waimai_10k/waimai_10k.csv -O data/waimai_10k.csv\n",
    "!pip install jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "tribal-cradle",
   "metadata": {
    "id": "tribal-cradle"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/workspace/CGUDL_2025_fall/.venv/lib/python3.12/site-packages/jieba/_compat.py:18: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\n"
     ]
    }
   ],
   "source": [
    "# 1. 導入所需套件\n",
    "\n",
    "# 第3方套件\n",
    "import jieba\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "floral-particular",
   "metadata": {
    "id": "floral-particular"
   },
   "outputs": [],
   "source": [
    "# 2. 以 pandas 讀取資料\n",
    "# 請先下載資料集\n",
    "\n",
    "df = pd.read_csv(\"./data/waimai_10k.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5764cbb8-a764-43a6-a84d-f20e5b38c209",
   "metadata": {
    "id": "5764cbb8-a764-43a6-a84d-f20e5b38c209"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>很快，好吃，味道足，量大</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>没有送水没有送水没有送水</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>非常快，态度好。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>方便，快捷，味道可口，快递给力</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>菜味道很棒！送餐很及时！</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label           review\n",
       "0      1     很快，好吃，味道足，量大\n",
       "1      1     没有送水没有送水没有送水\n",
       "2      1         非常快，态度好。\n",
       "3      1  方便，快捷，味道可口，快递给力\n",
       "4      1     菜味道很棒！送餐很及时！"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. 觀察資料\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c6f14d-298f-435f-b361-d418be65de83",
   "metadata": {
    "id": "55c6f14d-298f-435f-b361-d418be65de83"
   },
   "source": [
    "## 建立字典\n",
    "- 電腦無法僅透過字符來區分不同字之間的意涵\n",
    "- 電腦視覺領域依賴的是影像資料本身的像素值\n",
    "- 我們讓電腦理解文字的方法是透過向量\n",
    "- 文字的意義藉由向量來進行表達的形式稱為 word embeddings\n",
    "- 舉例:\n",
    "$\\textrm{apple}=[0.123, 0.456,0.789,\\dots,0.111]$\n",
    "\n",
    "- 如何建立每個文字所屬的向量？\n",
    "    - 傳統方法: 計數法則\n",
    "    - 近代方法 (2013-至今): 使用(淺層)神經網路訓練 word2vec ([參考](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/))，稱為 word embeddings\n",
    "    - 現代方法 (2018-至今): 使用(深層)神經網路訓練 Transformers，也就是BERT ([參考](https://youtu.be/gh0hewYkjgo))，又稱為 contexualized embeddings\n",
    "- 在那之前，要先建立分散式字詞的字典\n",
    "    - 可粗分兩種斷詞方式 (tokenization):\n",
    "        1. 每個字都斷 (character-level)\n",
    "        2. 斷成字詞 (word-level)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1b10a5-e5cc-4151-baf4-00852493ab6e",
   "metadata": {
    "id": "db1b10a5-e5cc-4151-baf4-00852493ab6e"
   },
   "source": [
    "## Word embeddings\n",
    "- 著名的方法有:\n",
    "    1. word2vec: Skip-gram, CBOW (continuous bag-of-words)\n",
    "    2. GloVe\n",
    "    3. fastText\n",
    "- 本教學使用 PyTorch 內建的 Embedding 層來實作 word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20845258",
   "metadata": {
    "id": "20845258"
   },
   "outputs": [],
   "source": [
    "word_to_idx = {\"<pad>\": 0, \"<unk>\": 1, \"好吃\": 2, \"棒\": 3, \"给力\": 4}\n",
    "embeds = torch.nn.Embedding(5, 5)  # 5 words in vocab, 5 dimensional embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6aec1001-2ee2-4b15-bf3f-588d3320c848",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_id(word, vocab, unk_idx: int = 1):\n",
    "    return vocab.get(word, unk_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23a1e9e0",
   "metadata": {
    "id": "23a1e9e0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.8612,  0.0705, -0.6171, -0.5732,  0.5950],\n",
      "        [ 1.4814,  1.0650, -3.2168,  0.4955, -0.1476],\n",
      "        [ 1.4814,  1.0650, -3.2168,  0.4955, -0.1476]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "lookup_tensor = torch.tensor(\n",
    "    [\n",
    "        get_word_id(\"好吃\", word_to_idx),\n",
    "        get_word_id(\"不棒\", word_to_idx),\n",
    "        get_word_id(\"<unk>\", word_to_idx),\n",
    "    ],\n",
    ")\n",
    "word_embed = embeds(lookup_tensor)\n",
    "print(word_embed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1fe18f",
   "metadata": {},
   "source": [
    "### 複習 torch.nn.Linear 用法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0fc699bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 30])\n"
     ]
    }
   ],
   "source": [
    "# torch.nn.Linear 用法範例\n",
    "# m 是一個線性轉換層，之前我們都在 forward 裡面使用它\n",
    "\n",
    "m = torch.nn.Linear(20, 30)\n",
    "input = torch.randn(128, 20) # 假設有 128 筆資料，每筆資料有 20 維\n",
    "output = m(input)\n",
    "\n",
    "print(output.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d20a4d6-103e-4e9e-8ca7-d18c7750c1be",
   "metadata": {
    "id": "4d20a4d6-103e-4e9e-8ca7-d18c7750c1be"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.591 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The vocab size is 11010.\n"
     ]
    }
   ],
   "source": [
    "# 4. 建立字典\n",
    "use_jieba=True\n",
    "\n",
    "vocab = {'<pad>':0, '<unk>':1}\n",
    "\n",
    "if use_jieba:\n",
    "    words = []\n",
    "    for sent in df['review']:\n",
    "        tokens = jieba.lcut(sent, cut_all=False)\n",
    "        words.extend(tokens)\n",
    "\n",
    "else:\n",
    "    # 以 character-level 斷詞\n",
    "    words = df['review'].str.cat()\n",
    "\n",
    "# 使字詞不重複\n",
    "words = sorted(set(words))\n",
    "for idx, word in enumerate(words):\n",
    "    # 一開始已經放兩個進去 dictionary 了\n",
    "    idx = idx + 2\n",
    "    # 將 word to id 放到 dictionary\n",
    "    vocab[word] = idx\n",
    "\n",
    "# 查看字典大小\n",
    "print(\"The vocab size is {}.\".format(len(vocab)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035a31d6-38d3-44f2-a98d-d487b2a90987",
   "metadata": {
    "id": "035a31d6-38d3-44f2-a98d-d487b2a90987"
   },
   "source": [
    "## 使用 PyTorch 建立 Dataset\n",
    "![Imgur](https://i.imgur.com/wGnfCmH.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e40cb6e1-bd26-44c2-886c-b3651f204d50",
   "metadata": {
    "id": "e40cb6e1-bd26-44c2-886c-b3651f204d50",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 5. 將資料分成 train/ validation/ test\n",
    "\n",
    "train_data, test_data = train_test_split(\n",
    "    df,\n",
    "    test_size=0.2,\n",
    ")\n",
    "train_data, validation_data = train_test_split(\n",
    "    train_data,\n",
    "    test_size=0.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c81e62ed-13ad-483c-a47a-0407d7b3ef35",
   "metadata": {
    "id": "c81e62ed-13ad-483c-a47a-0407d7b3ef35"
   },
   "outputs": [],
   "source": [
    "# 6. 定義超參數\n",
    "\n",
    "parameters = {\n",
    "    \"padding_idx\": 0,\n",
    "    \"vocab_size\": len(vocab),\n",
    "    # Hyperparameters\n",
    "    \"embed_dim\": 300,\n",
    "    \"hidden_dim\": 256,\n",
    "    \"module_name\": 'rnn', # 選項: rnn, lstm, gru, transformer\n",
    "    \"num_layers\": 2,\n",
    "    \"learning_rate\": 5e-4, # 使用 Transformer 時建議改成 5e-5\n",
    "    \"epochs\": 10,\n",
    "    \"max_seq_len\": 50,\n",
    "    \"batch_size\": 64,\n",
    "    \"bidirectional\": True,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "standard-joining",
   "metadata": {
    "id": "standard-joining"
   },
   "outputs": [],
   "source": [
    "# 7. 建立 PyTorch Dataset (定義 class)\n",
    "\n",
    "class WaimaiDataset(torch.utils.data.Dataset):\n",
    "    # 繼承 torch.utils.data.Dataset\n",
    "    def __init__(self, vocab, data, max_seq_len, use_jieba):\n",
    "        self.df = data\n",
    "        self.max_seq_len = max_seq_len\n",
    "        # 可以選擇要不要使用結巴進行斷詞\n",
    "        self.use_jieba = use_jieba\n",
    "        self.vocab = vocab\n",
    "        self.unk_idx = self.vocab.get('<unk>')\n",
    "\n",
    "    # 改寫繼承的 __getitem__ function\n",
    "    def __getitem__(self, idx):\n",
    "        # dataframe 的第一個 column 是 label\n",
    "        # dataframe 的第一個 column 是 評論的句子\n",
    "        label, sent = self.df.iloc[idx, 0:2]\n",
    "        # 先將 label 轉為 float32 以方便後面進行 loss function 的計算\n",
    "        label_tensor = torch.tensor(label, dtype=torch.float32)\n",
    "        if self.use_jieba:\n",
    "            # 使用 lcut 可以 return list\n",
    "            tokens = jieba.lcut(sent, cut_all=False)\n",
    "        else:\n",
    "            # 每個字都斷詞\n",
    "            tokens = list(sent)\n",
    "\n",
    "        # 控制最大的序列長度\n",
    "        tokens = tokens[:self.max_seq_len]\n",
    "\n",
    "        # 根據 vocab 轉換 word id\n",
    "        # 如果找不到該字詞，就用 <unk> 的 index 來表示\n",
    "        tokens_id = [self.vocab.get(word, self.unk_idx) for word in tokens]\n",
    "        tokens_tensor = torch.LongTensor(tokens_id)\n",
    "\n",
    "        # 所以 第 0 個index是句子，第 1 個index是 label\n",
    "        return tokens_tensor, label_tensor\n",
    "\n",
    "    # 改寫繼承的 __len__ function\n",
    "    def __len__(self):\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ee767846-421c-4eff-a4ff-3986308f1497",
   "metadata": {
    "id": "ee767846-421c-4eff-a4ff-3986308f1497"
   },
   "outputs": [],
   "source": [
    "# 8. 建立 PyTorch Dataset (執行 class)\n",
    "use_jieba=use_jieba\n",
    "\n",
    "trainset = WaimaiDataset(\n",
    "    vocab,\n",
    "    train_data,\n",
    "    parameters[\"max_seq_len\"],\n",
    "    use_jieba=use_jieba\n",
    ")\n",
    "validset = WaimaiDataset(\n",
    "    vocab,\n",
    "    validation_data,\n",
    "    parameters[\"max_seq_len\"],\n",
    "    use_jieba=use_jieba\n",
    ")\n",
    "testset = WaimaiDataset(\n",
    "    vocab,\n",
    "    test_data,\n",
    "    parameters[\"max_seq_len\"],\n",
    "    use_jieba=use_jieba\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3d98e478-770e-4d23-9f76-b20b3b88b906",
   "metadata": {
    "id": "3d98e478-770e-4d23-9f76-b20b3b88b906"
   },
   "outputs": [],
   "source": [
    "# 9. 整理 batch 的資料 (定義 function)\n",
    "\n",
    "def collate_batch(batch):\n",
    "    # 抽每一個 batch 的第 0 個(注意順序)\n",
    "    text = [i[0] for i in batch]\n",
    "    # 進行 padding\n",
    "    text = pad_sequence(text, batch_first=True)\n",
    "\n",
    "    # 抽每一個 batch 的第 1 個(注意順序)\n",
    "    label = [i[1] for i in batch]\n",
    "    # 把每一個 batch 的答案疊成一個 tensor\n",
    "    label = torch.stack(label)\n",
    "\n",
    "    return text, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4540bea8-399e-4759-ab63-351040f20042",
   "metadata": {
    "id": "4540bea8-399e-4759-ab63-351040f20042"
   },
   "outputs": [],
   "source": [
    "# 10. 建立資料分批 (mini-batches)\n",
    "\n",
    "# 因為會針對 trainloader 進行 shuffle\n",
    "# 對 trainloader 進行 shuffle 有助於降低 overfitting\n",
    "\n",
    "trainloader = DataLoader(\n",
    "    trainset,\n",
    "    batch_size=parameters[\"batch_size\"],\n",
    "    collate_fn=collate_batch,\n",
    "    shuffle=True,\n",
    ")\n",
    "validloader = DataLoader(\n",
    "    validset,\n",
    "    batch_size=parameters[\"batch_size\"],\n",
    "    collate_fn=collate_batch,\n",
    "    shuffle=False,\n",
    ")\n",
    "testloader = DataLoader(\n",
    "    testset,\n",
    "    batch_size=parameters[\"batch_size\"],\n",
    "    collate_fn=collate_batch,\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4670a6c-7747-423e-9008-293b44b8a79c",
   "metadata": {
    "id": "c4670a6c-7747-423e-9008-293b44b8a79c"
   },
   "source": [
    "## 建立模型\n",
    "![Imgur](https://i.imgur.com/OgLBBm7.png)\n",
    "- 模型建置的流程如上圖所示\n",
    "- 文字的部份會透過 Dataset 及 DataLoader 進行處理\n",
    "- embedding 層經由 nn.embedding 來實現 embedding lookup 的功能\n",
    "- embedding 層再接上模型，最後接上分類層，即可進行分類任務\n",
    "- 本範例提供的 Model class 可以藉由更換 module_name 來呼叫不同的 RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d077a1a8-b1e7-4f94-9c1e-001bb04cf5c4",
   "metadata": {
    "id": "d077a1a8-b1e7-4f94-9c1e-001bb04cf5c4"
   },
   "outputs": [],
   "source": [
    "# 11. 建立 RNN 模型 (定義 class)\n",
    "\n",
    "class RNNModel(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, padding_idx, bi=False):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.bidirectional = bi\n",
    "\n",
    "        # 定義 Embedding 層\n",
    "        self.embedding = torch.nn.Embedding(\n",
    "            num_embeddings=vocab_size,\n",
    "            embedding_dim=embed_dim,\n",
    "            padding_idx=padding_idx\n",
    "        )\n",
    "        # 定義 LSTM\n",
    "        self.rnn = torch.nn.LSTM(\n",
    "            input_size=embed_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            batch_first=True,\n",
    "            bidirectional=bi\n",
    "        )\n",
    "        # 根據是否 bidirectional 決定輸出層的輸入維度\n",
    "        direction_factor = 2 if bi else 1\n",
    "        self.fc = torch.nn.Linear(\n",
    "            # 如果 bi-directional，hidden_dim 是兩倍\n",
    "            in_features=hidden_dim * direction_factor,\n",
    "            out_features=1\n",
    "        )\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"定義神經網路的前向傳遞的進行流程\n",
    "        Arguments:\n",
    "            - X: 輸入值，維度為(B, S)，其中 B 為 batch size，S 為 sentence length\n",
    "        Returns:\n",
    "            - logits: 模型的輸出值，維度為(B, 1)，其中 B 為 batch size\n",
    "            - Y: 模型的輸出值但經過非線性轉換 (這邊是用 sigmoid)，維度為(B, 1)，其中 B 為 batch size\n",
    "        \"\"\"\n",
    "        # 維度: (B, S) -> (B, S, E)\n",
    "        # B: batch size; S: sentence length; E: embedding dimension\n",
    "        E = self.embedding(X)\n",
    "\n",
    "        # 使用 RNN 系列\n",
    "        H_out, (h_n, c_n) = self.rnn(E)\n",
    "\n",
    "        if self.bidirectional:\n",
    "            # 取第一個和最後一個 hidden states做相加 (bi-directional)\n",
    "            combined = torch.cat([H_out[:, -1, :self.hidden_dim],\n",
    "                                H_out[:, 0, self.hidden_dim:]], dim=1)\n",
    "        else:\n",
    "            # 取最後一個 hidden states (uni-directional)\n",
    "            combined = H_out[:, -1, :]\n",
    "\n",
    "        logits = self.fc(combined)\n",
    "        Y = torch.sigmoid(logits)\n",
    "\n",
    "        return logits, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8f580c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12. 執行訓練所需要的準備工作\n",
    "\n",
    "model = RNNModel(\n",
    "    vocab_size=parameters[\"vocab_size\"],\n",
    "    embed_dim=parameters[\"embed_dim\"],\n",
    "    hidden_dim=parameters[\"hidden_dim\"],\n",
    "    padding_idx=parameters[\"padding_idx\"],\n",
    "    bi=parameters[\"bidirectional\"],\n",
    ")\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=parameters[\"learning_rate\"])\n",
    "loss_func = torch.nn.BCEWithLogitsLoss() # 含有 sigmoid 的版本"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0fcde55-fd80-4995-bce2-cd5e2092d5e9",
   "metadata": {
    "id": "f0fcde55-fd80-4995-bce2-cd5e2092d5e9"
   },
   "source": [
    "## 設定訓練流程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "07b2579d-b998-4d48-9ad3-52f780823872",
   "metadata": {
    "id": "07b2579d-b998-4d48-9ad3-52f780823872"
   },
   "outputs": [],
   "source": [
    "# 13. 設定訓練流程 (定義 function)\n",
    "\n",
    "def train(trainloader, model, optimizer, loss_func):\n",
    "    \"\"\"定義訓練時的進行流程\n",
    "    Arguments:\n",
    "        - trainloader: 具備 mini-batches 的 dataset，由 PyTorch DataLoader 所建立\n",
    "        - model: 要進行訓練的模型\n",
    "        - optimizer: 最佳化目標函數的演算法\n",
    "    Returns:\n",
    "        - train_loss: 模型在一個 epoch 的 training loss\n",
    "    \"\"\"\n",
    "    # 設定模型的訓練模式\n",
    "    model.train()\n",
    "\n",
    "    # 記錄一個 epoch中 training 過程的 loss\n",
    "    train_loss = 0\n",
    "    # 從 trainloader 一次一次抽\n",
    "    for x, y in tqdm(trainloader, desc=\"Training\"):\n",
    "        # 將變數丟到指定的裝置位置\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        # 重新設定模型的梯度\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 1. 前向傳遞 (Forward Pass)\n",
    "        logits, pred = model(x)\n",
    "\n",
    "        # 2. 計算 loss (loss function 為二元交叉熵)\n",
    "        loss = loss_func(logits.squeeze(-1), y)\n",
    "\n",
    "        # 3. 計算反向傳播的梯度\n",
    "        loss.backward()\n",
    "        # 4. \"更新\"模型的權重\n",
    "        optimizer.step()\n",
    "\n",
    "        # 一個 epoch 會抽很多次 batch，所以每個 batch 計算完都要加起來\n",
    "        # .item() 在 PyTorch 中可以獲得該 tensor 的數值\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    return train_loss / len(trainloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ba00e6-0263-4b81-be42-359de3257859",
   "metadata": {
    "id": "a4ba00e6-0263-4b81-be42-359de3257859"
   },
   "source": [
    "## 設定驗證流程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e751e9b5-7628-4320-8b20-917bb0f3087f",
   "metadata": {
    "id": "e751e9b5-7628-4320-8b20-917bb0f3087f"
   },
   "outputs": [],
   "source": [
    "# 14. 設定驗證流程 (定義 function)\n",
    "\n",
    "def evaluate(dataloader, model, loss_func):\n",
    "    \"\"\"定義驗證時的進行流程\n",
    "    Arguments:\n",
    "        - dataloader: 具備 mini-batches 的 dataset，由 PyTorch DataLoader 所建立\n",
    "        - model: 要進行驗證的模型\n",
    "    Returns:\n",
    "        - loss: 模型在驗證/測試集的 loss\n",
    "        - acc: 模型在驗證/測試集的正確率\n",
    "    \"\"\"\n",
    "    # 設定模型的驗證模式\n",
    "    # 此時 dropout 會自動關閉\n",
    "    model.eval()\n",
    "    total_loss = 0 # 紀錄 loss 數值\n",
    "    label_list = []\n",
    "    prediction_list = []\n",
    "\n",
    "    # 設定現在不計算梯度\n",
    "    with torch.no_grad():\n",
    "        # 從 dataloader 一次一次抽\n",
    "        for x, y in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            logits, pred = model(x)\n",
    "\n",
    "            # 計算 loss (loss function 為二元交叉熵)\n",
    "            # 模型輸出的維度是 (B, 1)，使用.squeeze(-1)可以讓維度變 (B,)\n",
    "            loss = loss_func(logits.squeeze(-1), y)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # 預測的數值大於 0.5 則視為類別1，反之為類別0\n",
    "            pred = (pred > 0.5) * 1 # pred.shape: (B, 1)\n",
    "            prediction_list.extend(pred.cpu().squeeze(-1).tolist())\n",
    "            label_list.extend(y.cpu().tolist())\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    # 計算正確率\n",
    "    acc = accuracy_score(label_list, prediction_list)\n",
    "\n",
    "    return avg_loss, acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9557c9-c97d-4111-be93-2f256cd113f0",
   "metadata": {
    "id": "dd9557c9-c97d-4111-be93-2f256cd113f0"
   },
   "source": [
    "## 開始訓練"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "694f6cfd-9002-4fb5-8e98-f73e267970ba",
   "metadata": {
    "id": "694f6cfd-9002-4fb5-8e98-f73e267970ba"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 135/135 [00:02<00:00, 46.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss at epoch 1 is 0.4261981639597151.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 135/135 [00:02<00:00, 50.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss at epoch 2 is 0.2865349610646566.\n",
      "=====Start validation=====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 15/15 [00:00<00:00, 63.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy at epoch 2 is 0.8779979144942649, and validation loss is 0.29716561138629916.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 135/135 [00:02<00:00, 50.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss at epoch 3 is 0.22995638803199486.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 135/135 [00:02<00:00, 49.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss at epoch 4 is 0.18247239308224783.\n",
      "=====Start validation=====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 15/15 [00:00<00:00, 65.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy at epoch 4 is 0.8821689259645464, and validation loss is 0.3117324103911718.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 135/135 [00:02<00:00, 50.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss at epoch 5 is 0.1451969356448562.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 135/135 [00:02<00:00, 50.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss at epoch 6 is 0.11233452616466416.\n",
      "=====Start validation=====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 15/15 [00:00<00:00, 64.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy at epoch 6 is 0.881126173096976, and validation loss is 0.3674545675516129.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 135/135 [00:02<00:00, 50.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss at epoch 7 is 0.08731223189582428.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 135/135 [00:02<00:00, 49.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss at epoch 8 is 0.0654905130189878.\n",
      "=====Start validation=====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 15/15 [00:00<00:00, 65.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy at epoch 8 is 0.8759124087591241, and validation loss is 0.4049710800250371.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 135/135 [00:02<00:00, 50.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss at epoch 9 is 0.05956658816861886.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 135/135 [00:02<00:00, 49.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss at epoch 10 is 0.07818729467689992.\n",
      "=====Start validation=====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 15/15 [00:00<00:00, 65.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy at epoch 10 is 0.8644421272158499, and validation loss is 0.39072684148947395.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 15. 整個訓練及驗證過程的 script\n",
    "\n",
    "train_loss_history = []\n",
    "valid_loss_history = []\n",
    "\n",
    "for epoch in range(parameters[\"epochs\"]):\n",
    "    train_loss = train(\n",
    "        trainloader,\n",
    "        model,\n",
    "        optimizer=optimizer,\n",
    "        loss_func=loss_func,\n",
    "    )\n",
    "\n",
    "    print(\"Training loss at epoch {} is {}.\".format(epoch+1, train_loss))\n",
    "    train_loss_history.append(train_loss)\n",
    "\n",
    "    if epoch % 2 == 1:\n",
    "        print(\"=====Start validation=====\")\n",
    "        valid_loss, valid_acc = evaluate(\n",
    "            dataloader=validloader,\n",
    "            model=model,\n",
    "            loss_func=loss_func,\n",
    "        )\n",
    "        valid_loss_history.append(valid_loss)\n",
    "        print(\"Validation accuracy at epoch {} is {}, and validation loss is {}.\"\\\n",
    "              .format(epoch+1, valid_acc, valid_loss))\n",
    "\n",
    "    torch.save(model.state_dict(), \"model_epoch_{}.pkl\".format(epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "08cafd4e-a1e7-4fcf-9fdd-7dec429fb1d2",
   "metadata": {
    "id": "08cafd4e-a1e7-4fcf-9fdd-7dec429fb1d2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====Start testing=====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 38/38 [00:00<00:00, 63.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing accuracy is 0.8603002502085071.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 16. 預測測試集\n",
    "\n",
    "best_epoch = np.argmin(valid_loss_history)\n",
    "model.load_state_dict(\n",
    "    torch.load(\"model_epoch_{}.pkl\".format(best_epoch))\n",
    ")\n",
    "\n",
    "print(\"=====Start testing=====\")\n",
    "test_loss, test_acc = evaluate(testloader, model, loss_func)\n",
    "print(\"Testing accuracy is {}.\".format(test_acc))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
